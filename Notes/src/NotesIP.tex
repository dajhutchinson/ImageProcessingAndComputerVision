\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{changepage} 
\usepackage{longtable} 

\begin{document}

\pagestyle{fancy}
\setlength\parindent{0pt}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}

%New column type
\newcolumntype{L}[1]{>{\arraybackslash}m{#1cm}}

% Cover page title
\title{Image Processing and Computer Vision - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Image Processing and Computer Vision - Notes}
\fancyhead[R]{\today}

% Counters
\newcounter{definition}[section]
\newcounter{example}[section]
\newcounter{notation}[section]
\newcounter{proposition}[section]
\newcounter{remark}[section]
\newcounter{theorem}[section]
\newcounter{proof}[section]

% commands
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}
\newcommand{\cosech}[0]{\mathrm{cosech}\ }
\newcommand{\cosec}[0]{\mathrm{cosec}\ }
\newcommand{\sech}[0]{\mathrm{sech}\ }
\newcommand{\blocks}[0]{\mathbb{B}}
\newcommand{\nats}[0]{\mathbb{N}}
\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\integers}[0]{\mathbb{Z}}
\newcommand{\nb}[0]{\textit{N.B.} }
\newcommand{\ie}[0]{\textit{i.e.} }
\newcommand{\eg}[0]{\textit{e.g.} }

\newcommand{\definition}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}\\}
\newcommand{\example}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}\\}
\newcommand{\notation}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}\\}
\newcommand{\proposition}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}\\}
\newcommand{\remark}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}\\}
\newcommand{\theorem}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}\\}
\newcommand{\proof}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}\\}
\newcommand{\definitionn}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}}
\newcommand{\examplee}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}}
\newcommand{\notationn}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}}
\newcommand{\propositionn}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}}
\newcommand{\remarkk}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}}
\newcommand{\theoremm}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}}
\newcommand{\prooff}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}}


\tableofcontents

% Start of content
\newpage

\section{Image Acquistion}

\proposition{Common Challenges with Image Acquistion}
Below are some common challeges that are faced/produced by image acquistion\\
\begin{tabular}{|l|L{12}|}
\hline
Viewpoint Variation&Several images may be taken of the same object but will vary the angle\\
Illumination&Images may be taken in low/high light\\
Occlusion&Object may be partly obscured\\
Scale&Objects may look vary different when placed next to other objects due to their relative scale\\
Deformation&Objects may have slight variations on the perfect form\\
Background Clutter&Lots happening behind an object may work to obscure it\\
Object Intra-Class Variation&Some objects in the same class can vary a lot in shape (\eg chairs)\\
Local Ambiguity&Certain regions of an image can be missinturpred without the rest of the scene being accounted for\\
World Behind the Image&Depth may need to be accounted for to make sense of an image.\\
\hline
\end{tabular}\\

\definition{Dirac Delta-Function, $\delta$}
The \textit{Dirac Delta-Function} is used to map continuous distributions to discrete distributions by sampling at particular intervals. Intuitively
$$\delta(t)=\begin{cases}1&, t=0\\0&, t\neq0\end{cases}\implies\delta(t-\alpha)=\begin{cases}1&, t=\alpha\\0&, t\neq\alpha\end{cases}$$

\definition{Sifting Property}
We can apply the \textit{Dirac Delta-Function} to a function to sample a particular value
$$\int_{-\infty}^\infty f(t)\delta(t)dt=f(0)\implies\int_{-\infty}^\infty f(t)\delta(t-\alpha)dt=f(\alpha)$$
This can be applied to 2D objects (such as images) as
$$\int_{-\infty}^\infty f(a,b)\delta(a-x,b-y)dadb=f(x,y)$$

\definition{Point Spread-Function}
A \textit{Point Spread-Function} is applied after sampling an image. It takes the value of a pixel \& transforms pixels around it using this value in some way.\\
\eg\raisebox{-0.5\height}{\includegraphics[scale=1]{img/psf.png}} (Should be a white dot on black background but ink).

\section{Image Representation}

\definition{Colour Space}
\textit{Colour Space} are different techniques for representing colours. These are generally made up of 3D vectors.\\
\begin{tabular}{|l|L{13}|}
\hline
Colour Space&Vector Description\\
\hline
RGB&(Red $\in[0,255]$,Green $\in[0,255]$,Blue $\in[0,255]$)\\
HSI&(Hue $\in[0,360)$,Saturation $\in[0,1]$,Intensity $\in[0,1]$)\newline Hue gives the colour in degrees\\
YUV&(Brightness $\in[0,255]$,Blue Projection $\in[0,255]$,Red Projection $\in[0,255]$)\\
La*b*&(Luminance $\in[0,100]$, Red/Green $\in\{-a,+b\}$, Blue/Yellow $\in\{+b,-b\}$\\
\hline
\end{tabular}\\

\remark{Representing Video}
To represent video each fixel is given a third parameter, $time$ so we now have
$$f(x,y,t)\mapsto(R,G,B)$$
or any other \textit{Colour Space}.\\

\definition{Quantisation}
\textit{Quantisation} is representing a continuous single channel function with discrete single channel function that groups the continuous values into a set number of levels.\\

\example{Quantisation}
\includegraphics[scale=.3]{img/quantisation.png}

\definition{Aliasing}
\textit{Aliasing} is the result of sparse sampling since single pixels represent to large an area to get any detail out of it.\\

\example{Aliasing}
\includegraphics[scale=.4]{img/aliasing.png}

\definition{Anti-Aliasing}
\textit{Anti-Aliasing} is the process for avoiding \textit{Aliasing}. This can be achieved by using a sampling rate which is a critical limit defined by the \textit{Shannon-Nyquist Theorem}.\\

\theorem{Shannon-Nyquist Theorem}
An analogue signal with maximum frequency $x\text{Hz}$ may be completley reconstructed if regular samples are taken with frequency $2x\text{Hz}$.\\

\definition{Convolution}
\textit{Convolution} is an operation which takes two functions \& produces a third which describes how the shape of one of the two functions is changed by the other.\\
For functions $f\ \&\ g$
$$(f*g)(x):=\int_{-\infty}^\infty f(x-t)h(x)\partial t$$
\nb $*$ is the symbol for convolution.\\

\remark{Convolution in Image Representation}
Suppose you have a system, represented by kernel $g(x)$, \& an input signal, represented by $f(x)$. THen $f*g(x)$ describes the effect of the system on the input signal. The resulting image is called the \textit{Response of $f$ to the kernel $h$}.\\
%TODO what is a kernel

\proposition{2D Discrete Convolution}
Since images are represnted by discrete 2D functions $f:\nats\times\nats\to(\nats\times\nats\times\nats)$ it is pertinent to understand \textit{2D Discrete Convolution}.\\
$$h(x,y)=\sum_{i\in I}\sum_{j\in J}f(x-i,y-j)g(i,j)$$
Often the kernel, $g(x,y)$, has negative indices so the pixel being acted upto is equivalent to the middle pixel in the matrix representation of $g(x,y)$.\\
\nb A convolution whose kernel is symmetric on 180 degree rotation is called a \textit{Correlation}.\\

\example{2D Discrete Convolution}
Below is a representation of a grayscale image, $f(x,y)$, on the left \& a kernel $g(x,y)$ on the right.
\includegraphics[scale=.3]{img/2dconvolution.png}\\
$(f*g)(x,y)=f(x+1,y+1)g(-1,-1)+f(x+1,y)g(0-1,0)+\dots+f(x-1,y-1)g(1,1)=-68$.\\

\example{Kernels}
Kernels an be defined with specific outcomes in mind.\\
\begin{longtable}{|l|l|}
\hline
\textbf{Operation}&\textbf{Matrix}\\
\hline
Identity&$\begin{pmatrix}
0&0&0\\
0&1&0\\
0&0&0
\end{pmatrix}$\\
\hline
Edge Detection&$\begin{pmatrix}
1&0&-1\\
0&0&0\\
-1&0&1
\end{pmatrix}$\\
&$\begin{pmatrix}
0&1&0\\
1&-4&1\\
0&1&1
\end{pmatrix}$\\
&$\begin{pmatrix}
-1&-1&-1\\
-1&8&-1\\
-1&-1&-1
\end{pmatrix}$\\
\hline
Sharpen&$\begin{pmatrix}
0&-1&0\\
-1&5&-1\\
0&-1&0
\end{pmatrix}$\\
\hline
Box Blur&$\dfrac{1}{9}\begin{pmatrix}
1&1&1\\
1&1&1\\
1&1&1
\end{pmatrix}$
\\
\hline
Gaussian Blur $3\times3$&$\dfrac{1}{16}\begin{pmatrix}
1&2&1\\
2&4&2\\
1&2&1
\end{pmatrix}$\\
\hline
Gaussian Blur $5\times5$&$\dfrac{1}{256}\begin{pmatrix}
1&4&6&4&1\\
4&16&24&16&4\\
6&24&36&24&6\\
4&16&24&16&4\\
1&4&5&4&1
\end{pmatrix}$\\
\hline
Unsharp Masking $5\times5$&$\dfrac{-1}{256}\begin{pmatrix}
1&4&6&4&1\\
4&16&24&16&4\\
6&24&-476&24&6\\
4&16&24&16&4\\
1&4&6&4&1
\end{pmatrix}$\\
\hline
\end{longtable}

\section{Frequency Domains \& Image Transforms}

\definition{Image Transform}
An \textit{Image Transform} is deriving a new representation of the input data by encoding the image using another parameter space (\eg Fourier, DCT, Wavelet, etc.).\\

\remark{Purpose of Image Transforms}
\textit{Image Transforms} can be used in
\begin{enumerate}[label=\roman*)]
	\item Image Filtering;
	\item Image Compression;
	\item Feature Extraction;
	\item etc.
\end{enumerate}

\definition{Properties of a Signal}
A \textit{Signal} is a sinusoidal function over continuous time. They have the following properties
\begin{enumerate}[label=\roman*)]
	\item Frequency - Number of cycles per second, Hz;
	\item Period - Length of a cycle, s;
	\item Amplitude - Peak intensity of the signal;
	\item Phase - The shift of the trig wave from its default position, $\pi$.
\end{enumerate}

\theorem{Fourier's Theorem}
All periodic functions over continous time can be expressed as a sum of $\sin$ \&$ \cos$ terms, each with their own amplitude \& shift.\\
$$f(t)=a_0\sin(t+\theta_0)+a_1\cos(t+\theta_1)+\dots$$

\example{Fourier Transform}
\includegraphics[scale=2]{img/fourier_transform.jpg}

\proposition{Frequency in Images}
\textit{Frequency in Images} is measured as the rate of change in intensity along a given line on the image.\\

\remark{Fourier Transform on Frequency in Images}
If we read the intensity values along a single row or column we can produce a sinusoidal wave which generalises the distribution \& then perform a \textit{Fourier Transform}.\\

\definition{2D Discrete-Space Fourier Transform}
Images can be considered as 2-Dimensional discrete space. Let $f(x,y)$ be the intensity of the pixel at position $(x,y)$. 2D Discrete-Space Fourier Transforms have two variables: $u\in[-\pi,\pi)$ for the vertical frequency; and, $v\in[-\pi,\pi)$ for the horizontal frequency.
\[\begin{array}{rcl}
\underbrace{F(u,v)}_{\text{Fourier Space}}&=&{\displaystyle\sum_{y=0}^{m-1}\sum_{x=0}^{n-1}f(x,y)e^{i(ux+vy)}}\\
&=&{\displaystyle\sum_{y=0}^{m-1}\sum_{x=0}^{n-1}f(x,y)\left[\cos(ux+vy)+\sin(ux+vy)\right]}
\end{array}\]
\nb $F(u,v)$ is a complex number.\\

\proposition{Interpretations of 2D Fourier Transform}
Since $F(u,v)$ is a complex number we cannot plot it exactly. Thus we consider
\begin{enumerate}[label=\roman*)]
	\item $\text{Magnitude},\ |F(u,v)|:=\sqrt{F_\text{r}(u,v)^2+F_\text{i}(u,v)^2}$, and
	\item $\text{Phase Angles},\ \theta(u,v):=\tan^{-1}\left(\dfrac{F_\text{i}(u,v)}{F_\text{r}(u,v)}\right)$
\end{enumerate}

\remark{Expressing $F(u,v)$ in Polar Cordinates}
$$F(u,v)=|F(u,v)|e^{i\theta(u,v)}$$

\remark{Plotting Magnitude, $|F(u,v)|$}
On the left we have a gray scale image \& on the right we have the magnitude of a fourier transform on this image. On the right hand image the $y$-axis is $u\in[-\pi,\pi)$ and the $x$-axis is $v\in[-\pi,\pi)$. We see lots of straight lines since a linear transformation on $F(u,v)=F(au,av)\ \forall\ a\in\mathbb{R}$. Each line can be interpreted as the frequency of intensity for lines in the left hand image which are parallel to it.\\
\includegraphics[scale=3]{img/fourier_transform_magnitude.jpg}\\
%TODO interpretting

\theorem{Convolution Theorem}
Let $f$ be an image, $g$ be a kernel, $F$ be the result of a fourier transform on $f$ and $G$ be a kernel. Then
$$h=f*g\Longleftrightarrow H=FG$$

\proof{Convolution Theorem}
\[\begin{array}{rcl}
h(x)&=&f(x)*g(x)\\
&=&\sum_yf(x-y)g(y)\\
H(u)&=&\sum_x\left(\sum_yf(x-y)g(y)\right)e^{iux}\\
&=&\sum_xg(y)\sum_xf(x-y)e^{iux}\\
&=&\sum_yg(y)\left(F(u)e^{iux}\right)\\
&=&\sum_yg(y)e^{iuy}F(u)\\
&=&G(u)\cdot F(u)\\
&=&F(u)\cdot G(u)
\end{array}\]

\definition{Butterworth's Low Pass Filter}
\textit{Butterworth's Low Pass Filter} is a \textit{Signal Processing Filter} designed to have a frequency response which is as flat as possible. It appears to soften an image
$$H(u,v)=\frac{1}{1+\left(\dfrac{r(u,v)}{r_0}\right)^{2n}}\text{ of order $n$}$$

\definition{Butterworth's High Pass Filter}
\textit{Butterworth's High Pass Filter} is a \textit{Signal Processing Filter} designed to have a frequency response which is as flat as possible. It appears to sharpen an image
$$H(u,v)=\frac{1}{1+\left(\dfrac{r_0}{r(u,v)}\right)^{2n}}\text{ of order $n$}$$

\section{Edges \& Shapes}

\subsection{Edge Detection}

\remark{Edges are Useful}
Edges are one of the best features to identify an object with. This \textit{Edge-Detection} is a useful thing to be able to do.

\definition{Edge}
An \textit{Edge} is a sharp change in image brightness. Edges are produced by object boundaries, patterns \& shadows, this can lead to us finding \textit{Nusiance Edges} which do not help achieve our goal.\\

\example{Edge Detection}
\includegraphics[scale=.7]{img/edgeDetection.png}

\remark{Uses of Edges}
\textit{Edges} are used for
\begin{enumerate}[label=\roman*)]
	\item Segmentation - Finding object boundaries;
	\item Recognition - Extracting patterns;
	\item Motion Analysis - Finding reliable tracking regions.
\end{enumerate}

\remark{Edge Detection Strategy}
In order to find edges we want to determine the \textit{rate of change} in a pixel's neighbourhood

\definition{Image Gradient}
\textit{Image Gradient} is a vector which gives the direction of greatest change in intensity at a specific pixel. For a pixel at $(x,y)$ with $f:\reals\times\reals\to\reals$ mapping to the intensity the \textit{Image Gradient} is
$$\nabla f(x,y):=\begin{pmatrix}
\frac{\partial f}{\partial x}\\
\frac{\partial f}{\partial y}
\end{pmatrix}$$
\nb Generally denoted by $\Psi$.\\

\definition{Angle of Gradient}
$$\Psi:=\tan^{-1}\left(\dfrac{\partial f/\partial y}{\partial f/\partial x}\right)$$

\definition{Edge Direction}
\textit{Edge Direction} for a specific pixel is the direction of an edge, it is perprendicular/orthogonal to the \textit{Image Gradient}.\\
\nb Generally denoted by $\Phi$.\\
$$\Phi:=\Psi-\frac{\pi}{2}=\tan^{-1}\left(\dfrac{\partial f/\partial y}{\partial f/\partial x}\right)-\dfrac{\pi}{2}$$

\definition{Image Gradient Magnitude}
\textit{Magnitude} measures the magnitude of the growth.
$$|\nabla f(x,y)|:=\sqrt{\left(\dfrac{\partial f}{\partial x}\right)^2+\left(\dfrac{\partial f}{\partial y}\right)^2}$$

\proposition{Estimating $\nabla f$}
We can define $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ to be matrices, rather than typical derivatives, in order to produce estimates for the \textit{Image Gradient} which can be used to analyse an image by convolution. We can then combine the results in order to analyse \textit{Image Gradient Angle} and \textit{Magnitude}. Consider
$$\dfrac{\partial f}{\partial x}=\begin{pmatrix}-1&0&1\\-1&0&1\\-1&0&1\end{pmatrix}\text{ and }\dfrac{\partial f}{\partial y}=\begin{pmatrix}-1&-1&-1\\0&0&0\\1&1&1\end{pmatrix}$$
\includegraphics[scale=.5]{img/gradientExtractionByFiltering.png}

\definition{Prewitt Operator}
\textit{Prewitt Operator}s are a set of $3\times3$ kernels used for estimating the \textit{Image Gradient}. There are 8 \textit{Prewitt Operators}, each in a different direction\\
\begin{tabular}{|c|c|c|c|}
\hline
$\begin{pmatrix}
1&1&1\\
0&0&0\\
-1&-1&-1
\end{pmatrix}$&
$\begin{pmatrix}
0&1&1\\
-1&0&1\\
-1&-1&0
\end{pmatrix}$&
$\begin{pmatrix}
-1&0&1\\
-1&0&1\\
-1&0&1
\end{pmatrix}$&
$\begin{pmatrix}
-1&-1&0\\
-1&0&1\\
0&1&1
\end{pmatrix}$\\
\hline
$\begin{pmatrix}
-1&-1&-1\\
0&0&0\\
1&1&1
\end{pmatrix}$&
$\begin{pmatrix}
0&-1&-1\\
1&0&-1\\
1&1&0
\end{pmatrix}$&
$\begin{pmatrix}
1&0&-1\\
1&0&-1\\
1&0&-1
\end{pmatrix}$&
$\begin{pmatrix}
1&1&0\\
1&0&-1\\
0&-1&-1
\end{pmatrix}$\\
\hline
\end{tabular}\\

\remark{Alternative estimates of $\nabla f$}
Instead of weighting all pixels in a given direction equally, we may want to weight central ones more heavily. This is since the central pixels are geometrically closer to the pixel which is being acted upon \& such should better describe it.
$$\dfrac{\partial f}{\partial x}=\begin{pmatrix}
-1&0&1\\
-2&0&2\\
-1&0&1
\end{pmatrix}\quad\dfrac{\partial f}{\partial y}=\begin{pmatrix}
1&2&1\\
0&0&0\\
-1&-2&-1
\end{pmatrix}$$
\nb This is called the \textit{Sobel Filter}.

\subsection{Shapes}

\definition{Hough Transform}
A \textit{Hough Transform} is used to find lines which best explain the set of edge points given to it. \textit{Hough Transform}s are centred around the premise that lines can be described by ${\rho=x\cos\theta+y\sin\theta}$ where either $(x,y)$ is fixed, or $(\rho,\theta)$ is fixed.\\
Suppose we are given an edge point $(x_0,y_0)$, we are now dealing with the case where $(\rho,\theta)$ are variable. Plot all the combinations of $(\rho,\theta)$ than produce lines which pass through $(x_0,y_0)$ produces a sinusoidal wave.\\
\includegraphics[scale=.7]{img/houghTransform.png}

\proposition{Line Detection Algorithm}
Below is an algorithm which uses the \textit{Hough Transform} on a set of edge points in order to detect lines in an image
\begin{enumerate}[label=\roman*)]
	\item Make available an $n=2$ dimensional array, $H(\rho,\theta)$, to be used for the parameter space;
	\item Find the \textit{Gradient Image}, $G(x,y)$;
	\item For any pixel $(x_0,y_0)$ where $|G(x_0,y_0)|>T_s$ (some threshold value) increment all values of $H(\rho,\theta)$ where $(\rho,\theta)$ satisfy $\rho=x_0\cos\theta_0+y_0\sin\theta_0$.
	$$\forall\ \rho,\theta\text{ where }\rho=x_0\cos\theta_0+y_0\sin\theta_0\text{ do }H(\rho,\theta)+=1$$
	\item Any $(\rho,\theta)$ where $H(\rho,\theta)>=T_h$ (another threshold value) represent a straight line which has been detected in the image.
\end{enumerate}

\propositionn{Circle Detection Algorithm}
\begin{enumerate}[label=\roman*)]
	\item Make available an $n=3$ dimensional array, $H(x,y,r)$, to be used for the parameter space;
	\item Find the \textit{Gradient Image}, $G(x,y)$;
	\item For any pixel $(x_0,y_0)$ where $|G(x_0,y_0)|>T_s$ increment all $H(x,y,r)$ which satisfy
	$$\forall\ r\text{ where }x_0=x+r\cos\Phi\text{ and }y_0+y+r\sin\Phi$$
	\item Any $(x,y,r)$ where $H(x,y,r)>T_h$ represents a circle with radius $r$ and centre $(x_0,y_0)$
\end{enumerate}

\definition{General Hough Transform}
A \textit{Genereal Hough Transform} is used to describe general shapes.
\begin{enumerate}[label=\roman*)]
	\item Find the \textit{Gradient Image}, $G(x,y)$.
	\item Define a set of points $\phi_i:=\frac{pi}{i}$ for $i\in[1,k]$ and create a table using $\phi_1,\dots,\phi_k$ as indexes.
	\item Define a reference point $(x_c,y_c)$ to act as a \textit{centre of mass}.
	\item For any given edge point $(x_0,y_0)$ find
	$$r=\sqrt{(x_0-x_c)^2+(y_0-y_c)^2},\ \beta=\tan^{-1}\left(\dfrac{y_0-y_c}{x_0-x_c}\right)\text{ and }\Phi_{(x_0,y_0)}\text{ in }G(x,y)$$
	\item Round $\Phi_{(x_0,y_0)}$ to the nearest $\phi_i$ and insert $(r,\beta)$ into the table at $\phi_i$
\end{enumerate}

\propositionn{General Shape Detection Algorithm}
\begin{enumerate}[label=\roman*)]
	\item Prepare $2$ dimensional array $H(x_c,y_c)$ for the parameter space (Set of all possible centre of masses).
	\item $\forall\ (x_0,y_0)$ where $|G(x_0,y_0)|>T_s$ find the table entry, $\phi_i$, closest to $\Phi_{(x_0,y_0)}$.
	\item $\forall\ (r_j,\beta_j)$ in the table entry find
	$$x_c=x+r\cos\beta\text{ and }y_c=y+r\sin\beta$$
	Increment $H(x_c,y_c)$.
	\item All $(x_c,y_c)$ where $H(x_c,y_c)>T_h$ represents the locations of centre of masses for occurances of the shap, in the image.
\end{enumerate}

\remark{Issues with \textbf{Proposition 4.4}}
The algorithm defined in \textbf{Proposition 4.4} only detects shapes of the same \textit{scale} and \textit{orientation} as the original image. To counter this we introduce two new variables to the parameter space: $S$, a scaling factor; and, $\theta$, an orientation factor.
\begin{enumerate}[label=\roman*)]
	\item Prepare $4$ dimensional array $H(x_c,y_c,S,\theta)$ for the parameter space (Set of all possible centre of masses, scales \& orientations).
	\item $\forall\ (x_0,y_0)$ where $|G(x_0,y_0)|>T_s$ find the table entry, $\phi_i$, closest to $\Phi_{(x_0,y_0)}$.
	\item $\forall\ (r_j,\beta_j)$ in the table entry find
	$$x_c=x+rS\cos(\beta+\theta)\text{ and }y_c=y+rS\sin(\beta+\theta)$$
	Increment $H(x_c,y_c,S,\theta)$.
	\item All $(x_c,y_c)$ where $H(x_c,y_c,S,\theta)>T_h$ represents the locations of centre of masses, scaling factors \& orientations for occurances of the shape in the image.
\end{enumerate}

\section{Image Segmentation}

\definition{Image Segmentation}
\textit{Image Segmentation} is the process of partitioning the pixels in an image into homogenous regions. This may wrt colour, texture, object.\\

\remark{Usefulness of Image Segmentation}
\textit{Image Segmentation} is useful since it simplifies an image from millions of pixels into a few regions making computation much easier.\\

\definition{Gestalt Rules for Grouping \& Segmentation}
It is useful to consider the different ways in which we can group objects.\\
\includegraphics[scale=.5]{img/gestalt.png}

\remark{Perfect Image Segmentation is hard}
It is hard to achieve perfect \textit{Image Segmentation} since pixels may straddle the boundary of objects \& thus not truely belong to any object. Noise, non-uniform illumination, occlusions etc. will also cause problems.\\

\definition{Over-Segmentation}
\textit{Over-Segmentation} is when an image is segmented into too many regions \& loses the efficienies gained by the simplication of \textit{Image Segmentation}.\\

\example{Over-Segmentation}
\includegraphics[scale=1]{img/overSegmentation.png}

\definition{Under-Segmentation}
\textit{Uner-Segmentation} is when an image is segmented into too few regions \& too much information is lost.\\

\example{Under-Segmentation}
\includegraphics[scale=1]{img/underSegmentation.png}

\proposition{Segmentation Technique Types}
There are several groups of techniques that can be used for \textit{Image Segmentation}
\begin{enumerate}[label=\roman*)]
	\item Thresholding. Categorise pixels wrt intensity.
	\item Edge-Based. Region boundaries are produced using an edge map.
	\item Region-Based. Regions are grown from seed pixels or using split-merge techniques.
	\item Clustering \& Statistical. Global partioning, oftern based around histograms. (\eg K-means).
	\item Topographic (OUT OF SCOPE). Stepwise simplications that take spatially wider image configurations into account.
\end{enumerate}

\proposition{Thresholding Image Segmentation}
\textit{Thresholding} is a good technique when trying to distinguish dark objects(background) from bright objects (foreground). One possible algorithm is
\begin{enumerate}[label=\roman*)]
	\item Choose a threshold value $T$.
	\item For each pixel:
	\begin{enumerate}
		\item If the brighness is less than $T$ map to $0$ (black).
		\item else, map to $255$ (white).
	\end{enumerate}
\end{enumerate}

\remark{Choosing Threshold Value}
Choosing a value for \textit{Thresholding} is important since if it is too high then the background pixels are classified as the foreground. And visa-versa for too low.\\

\proposition{Choosing Threshold Value}
There are several techinques used for choosing \textit{Threshold Value}. By consider the histogram of an image's intensity we can produce regions for classification.
\begin{enumerate}[label=\roman*)]
	\item Make an initial estimate for threshold $T$.
	\item Segment the image using $T$: Let $G_1$ be the pixels with intensity $>T$; and $G_2$ be the pixels with intensity $<T$.
	\item COmpute the average intensity values $m_1$ \& $m_2$ for $G_1$ \& $G_2$ respectively.
	\item Compute a new threshold value $T=\frac{1}{2}(m_1+m_2)$.
	\item Repeat \textbf{ii)-iv)} until convergence.
\end{enumerate}

\proposition{Edge-Based Segmentation}
When performing \textit{Edge-Based Image Segmentation} we want to detect strong edges that are relevant to the objet we wish to detect. There are several techniques for assessing the strength of an edge, but generally we wish to remove edges that are isolated of irrelevant to an object. One valid technique is edge relaxation where we change the strength of an edge depending upon its 6 neighbour edges.\\
\includegraphics[scale=.7]{img/edgeRelaxation.png}.\\

\proposition{Region Growing-Seed Pixel}
A function is required to analyse the homogeneity of two pixels $(s,p)\to\{0,1\}$.
\begin{enumerate}[label=\roman*)]
	\item Choose an initial seed pixel;
	\item Consider the neighbouring pixels:
	\begin{enumerate}
		\item If it is close to seed pixel, add to region \& to queue to be analysed.
		\item Else, do nothing with it.
	\end{enumerate}
	\item Repeat \textbf{ii)} until there are no pixels left in the queue to be analysed.
\end{enumerate}
\nb You may wish to add a final step to remove small regions.\\

\proposition{Split \& Merge Segmentation}
Let $H$ be a function to analyse the homogeneity of a region $(R)\to\{0,1\}$
\begin{enumerate}[label=\roman*)]
	\item Let $R_0$ represent the entire image.
	\item If $H(R_i)=0$ (\ie is inhomogenous) then split it into four regions.
	\item Repeat \textbf{ii)} until all regions are homogenous.
	\item Merge all subregions that satisfy $H(R_i\cup R_j)=1$ (\ie are homogeneous)
\end{enumerate}

\proposition{Clustering Segmentation}
If we map the RGB values of an image into the 3D real space we can we clustering algorithms to produce $k$ clusters. We then map back to the pixel space using each cluster as a segment.\\
\nb Look at k-means clustering.

\newpage
\setcounter{section}{-1}

\section{Reference}

\subsection{Definitions}

\definition{Connectivity}
When analysing a pixel there are several ways to consider its adjacent neighbourhood.
\begin{itemize}
	\item[-] 4-Connectivity. The pixels above, below, left \& right.
	\item[-] 8-Connectivity. Above, below, left, right \& single step diagonals.
\end{itemize}

\definition{Kernel}
A \textit{Kernel} is a small matrix used in convolution. Typically $3\times3$ or $5\times5$. \textit{Kernels} can be defined for blurring, sharpening, embossing, edge detection \& more
\nb This definition only applies to image processing \& is different from the definition in linear algebra.

\end{document}